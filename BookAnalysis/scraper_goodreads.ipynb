{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Vg4_adSsUiz"
      },
      "outputs": [],
      "source": [
        "#Pandas, requests, BeautifulSoup used for web scraping\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Identify user for probing websites\n",
        "user_agent = \"bookworm\""
      ],
      "metadata": {
        "id": "QMnX0-2jsh47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "book_titles = []\n",
        "authors = []\n",
        "avg_ratings = []\n",
        "ratings = []\n",
        "published_years = []\n",
        "editions = []\n",
        "desc = []\n",
        "\n",
        "#title -> Book Name\n",
        "#Rating -> Book rating given by the user\n",
        "#Genre -> Category(Type of book).\n",
        "#Author -> Book Author\n",
        "#Desc -> Book description\n",
        "#url -> Book cover image link"
      ],
      "metadata": {
        "id": "t3T46wzispHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages_to_scrape = 10\n",
        "\n",
        "#Delay between page requests\n",
        "request_delay = 2\n",
        "\n",
        "#Iterate all of the pages\n",
        "for page in range(1, pages_to_scrape + 1):\n",
        "\n",
        "    # Build the URL for each page\n",
        "    url = \"https://www.goodreads.com/search?page=\" + str(page) + \"&q=classics&qid=cB8CbxWplQ&search_type=books&tab=books&utf8=âœ“\"\n",
        "\n",
        "    try:\n",
        "        #HTTP GET request passed to URL provided the useragent\n",
        "        headers = {\"User-Agent\": user_agent}\n",
        "        response = requests.get(url, headers=headers).text\n",
        "\n",
        "        #Grab all of the response data using BeautifulSoup\n",
        "        soup = BeautifulSoup(response, \"html5lib\")\n",
        "\n",
        "        #Avoid errors if 404\n",
        "        if soup.title and \"service unavailable\" in soup.title.text.lower():\n",
        "            print(f\"Server error on page {page}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        #Table that has all of the books listed\n",
        "        table = soup.find_all(\"tbody\")[0]\n",
        "\n",
        "        for row in table.find_all(\"tr\"):\n",
        "            cells = row.find_all(\"td\")[1]\n",
        "\n",
        "            # get book title\n",
        "            title = cells.find(\"a\").find(\"span\").text\n",
        "            book_titles.append(title)\n",
        "\n",
        "            # get author\n",
        "            author = cells.find(\"a\", class_=\"authorName\").text\n",
        "            authors.append(author)\n",
        "\n",
        "\n",
        "            # rating\n",
        "            all_ratings = cells.find_all('span', class_ = 'minirating')\n",
        "            all_ratings_text = all_ratings[0].text.strip()\n",
        "            pattern_2 = re.compile(r\"(\\d\\.?\\d*)\\savg\")\n",
        "            avg_ratings.append(pattern_2.search(all_ratings_text).group(1))\n",
        "\n",
        "            # n_ratings  count how many total ratings\n",
        "            pattern_4 = re.compile(r\"(\\d\\,?\\d*) rating\")\n",
        "            ratings_matches = pattern_4.search(all_ratings_text)\n",
        "            ratings.append(ratings_matches.group(1) if ratings_matches else 0)\n",
        "\n",
        "\n",
        "            # published year and handles edge cases for improper format\n",
        "            year_info = cells.find(\"span\", class_=\"greyText smallText uitext\").text.split()\n",
        "            year = None\n",
        "            for item in year_info:\n",
        "                if item.isdigit() and len(item) == 4:\n",
        "                    year = item\n",
        "                    break\n",
        "            if year:\n",
        "                published_years.append(year)\n",
        "            else:\n",
        "                published_years.append(0)\n",
        "\n",
        "            #edition information\n",
        "            edition = cells.find(\"span\", class_=\"greyText smallText uitext\").text.split()[-2]\n",
        "            editions.append(edition)\n",
        "\n",
        "        # sleep on delay period\n",
        "        time.sleep(request_delay)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        # HTTP request errors\n",
        "        print(f\"Error on page {page}: {e}\")\n",
        "\n",
        "    except IndexError as e:\n",
        "        # list index out of range\n",
        "        print(f\"Index error on page {page}: {e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        #handle other unexpected errors\n",
        "        print(f\"Unexpected error on page {page}: {e}\")"
      ],
      "metadata": {
        "id": "KtT5jUCysxAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dataFrame\n",
        "data = {\n",
        "    \"Title\": book_titles,\n",
        "    \"Author\": authors,\n",
        "    \"Average Rating\": avg_ratings,\n",
        "    \"Rating\": ratings,\n",
        "    \"Year Published\": published_years,\n",
        "    \"Edition\": editions\n",
        "}"
      ],
      "metadata": {
        "id": "bVCeVkYptMnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "goodreads = pd.DataFrame(data)\n",
        "\n",
        "# Check if there are reviews in the DataFrame and no errors\n",
        "if not goodreads.empty:\n",
        "    # Specify the file path where you want to save the CSV file\n",
        "    csv_file_path = \"book_reviews.csv\"\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    goodreads.to_csv(csv_file_path, index=False)\n",
        "\n",
        "    print(f\"CSV file saved to {csv_file_path}\")\n",
        "else:\n",
        "    print(\"No reviews to save or an error occurred during scraping.\")\n",
        "\n",
        "    #https://raw.githubusercontent.com/sdhilip200/Content-Based-Recommendation---Good-Reads-data/master/data.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zShdSGcJtRbn",
        "outputId": "f8ea4476-317d-4dbc-e3b3-9c49e4166022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file saved to book_reviews.csv\n"
          ]
        }
      ]
    }
  ]
}